<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <!--#include virtual="/template/header.html"-->
  <title>Basic Guide to Program Evaluation (Including Many Additional Resources)</title>
  <meta name="description" content="Get a basic guide to program evaluation in this topic from the Free Management Library." />
</head>
<body bgcolor="#ffffff">

<!--#include virtual="/template/top-left.html"-->

<h1>Basic Guide to Program Evaluation (Including Outcomes Evaluation)</h1>

<!--[index|Evaluations (Many Kinds)]-->

<p>&copy; Copyright <a href="http://www.authenticityconsulting.com/"
target="_blank">Carter McNamara, MBA, PhD, Authenticity Consulting,
LLC</a>.<br />
Adapted from the <a href="http://www.authenticityconsulting.com/pubs/PG_gdes/PG_pubs.htm"
target="_blank">Field Guide to Nonprofit Program Design, Marketing
and Evaluation</a>.</p>

<p><a name="anchor487705"></a></p>

<p>This document provides guidance toward planning and implementing
an evaluation process for for-profit or nonprofit programs --
there are many kinds of evaluations that can be applied to programs,
for example, goals-based, process-based and outcomes-based. Nonprofit
organizations are increasingly interested in outcomes-based evaluation.
If you are interested in learning more about outcomes-based evaluation,
then see the sections <a href="#anchor3873957">Outcomes-Evaluation</a>
and <a href="http://managementhelp.org/evaluation/outcomes-evaluation-guide.htm"
class="textlink">Outcomes-Based Evaluations in Nonprofit Organizations</a>.</p>

<h2>Sections of This Topic Include</h2>

<p>
<a href="#anchor1575679">Program Evaluation: carefully getting
information to make decisions about programs</a><br />
<a href="#anchor1577333">Where Program Evaluation is Helpful</a><br />
<a href="#anchor1579318">Basic Ingredients (you need an organization
and program(s))</a><br />
<a href="#anchor1578833">Planning Program Evaluation (what do
you want to learn about, what info is needed)</a><br />
<a href="#anchor1581634">Major Types of Program Evaluation (evaluating
program processes, goals, outcomes, etc.)</a><br />
<a href="#anchor1585345">Overview of Methods to Collect Information
(questionnaires, interviews, focus groups, etc.)</a><br />
<a href="#anchor1665834">Selecting Which Methods to Use (which
methods work best to get needed info from audiences)</a><br />
<a href="#anchor1316141">Analyzing and Interpreting Information
</a><br />
<a href="#anchor4293321196">Reporting Evaluation Results</a><br />
<a href="#anchor4294569952">Who Should Carry Out the Evaluation?</a><br />
<a href="#anchor1586742">Contents of an Evaluation Plan</a><br />
<a href="#anchor1587540">Pitfalls to Avoid</a></p>

<p>
<a href="#anchor54478">Online Guides, etc.</a><br />
<a href="#anchor3873957">Outcomes-Evaluation</a><br />
<a href="#anchor227968">General Resources</a></p>

<p><b><i>Also see</i></b><br />
  <a href="http://managementhelp.org/evaluation/index.htm">Evaluations (many 
  kinds)</a><br />
  <a href="#anchor1029384756">Related Library Topics</a></p>
<p><a href="#anchor1029384756">Related Library Topics</a></p>
<h3>Also See the Library's Blogs Related to Program Evaluations</h3>
<p>In addition to the articles on this current page, see the following blogs which 
  have posts related to Program Evaluations. Scan down the blog's page to see 
  various posts. Also see the section &quot;Recent Blog Posts&quot; in the sidebar 
  of the blog or click on &quot;next&quot; near the bottom of a post in the blog.</p>
<p> <a href="http://managementhelp.org/blogs/business-planning/" target="_blank">Library's Business 
  Planning Blog</a><br />
  <a href="http://managementhelp.org/blogs/building-a-business/" target="_blank">Library's Building 
  a Business Blog</a><br />
  <a href="http://managementhelp.org/blogs/strategic-planning/" target="_blank">Library's Strategic 
  Planning Blog</a></p>
<hr align="left" />

<h3>A Brief Introduction ...</h3>

<p>Note that the concept of program evaluation can include a wide
variety of methods to evaluate many aspects of programs in nonprofit
or for-profit organizations. There are numerous books and other
materials that provide in-depth analysis of evaluations, their
designs, methods, combination of methods and techniques of analysis.
However, personnel do not have to be experts in these topics to
carry out a useful program evaluation. The &quot;20-80&quot; rule
applies here, that 20% of effort generates 80% of the needed results.
It's better to do what might turn out to be an average effort
at evaluation than to do no evaluation at all. (Besides, if you
resort to bringing in an evaluation consultant, you should be
a smart consumer. Far too many program evaluations generate information
that is either impractical or irrelevant -- if the information
is understood at all.) This document orients personnel to the
nature of program evaluation and how it can be carried out in
a realistic and practical fashion.</p>

<p>Note that much of the information in this section was gleaned
from various works of Michael Quinn Patton.</p>


<hr align="left" />
<a name="anchor1575679"></a>

<h2>Program Evaluation</h2>

<h3>Some Myths About Program Evaluation</h3>

<p>1.. Many people believe evaluation is a useless activity that
generates lots of boring data with useless conclusions. This was
a problem with evaluations in the past when program evaluation
methods were chosen largely on the basis of achieving complete
scientific accuracy, reliability and validity. This approach often
generated extensive data from which very carefully chosen conclusions
were drawn. Generalizations and recommendations were avoided.
As a result, evaluation reports tended to reiterate the obvious
and left program administrators disappointed and skeptical about
the value of evaluation in general. More recently (especially
as a result of Michael Patton's development of utilization-focused
evaluation), evaluation has focused on utility, relevance and
practicality at least as much as scientific validity.</p>

<p>2. Many people believe that evaluation is about proving the
success or failure of a program. This myth assumes that success
is implementing the perfect program and never having to hear from
employees, customers or clients again -- the program will now
run itself perfectly. This doesn't happen in real life. Success
is remaining open to continuing feedback and adjusting the program
accordingly. Evaluation gives you this continuing feedback.</p>

<p>3. Many believe that evaluation is a highly unique and complex
process that occurs at a certain time in a certain way, and almost
always includes the use of outside experts. Many people believe
they must completely understand terms such as validity and reliability.
They don't have to. They do have to consider what information
they need in order to make current decisions about program issues
or needs. And they have to be willing to commit to understanding
what is really going on. Note that many people regularly undertake
some nature of program evaluation -- they just don't do it in
a formal fashion so they don't get the most out of their efforts
or they make conclusions that are inaccurate (some evaluators
would disagree that this is program evaluation if not done methodically).
Consequently, they miss precious opportunities to make more of
difference for their customer and clients, or to get a bigger
bang for their buck.</p>

<h3>So What is Program Evaluation?</h3>

<p>First, we'll consider &quot;what is a program?&quot; Typically,
organizations work from their mission to identify several overall
goals which must be reached to accomplish their mission. In nonprofits,
each of these goals often becomes a program. Nonprofit programs
are organized methods to provide certain related services to constituents,
e.g., clients, customers, patients, etc. Programs must be evaluated
to decide if the programs are indeed useful to constituents. In
a for-profit, a program is often a one-time effort to produce
a new product or line of products. <br />
<br />
So, still, what is program evaluation? Program evaluation is carefully
collecting information about a program or some aspect of a program
in order to make necessary decisions about the program. Program
evaluation can include any or a variety of at least 35 different
types of evaluation, such as for needs assessments, accreditation,
cost/benefit analysis, effectiveness, efficiency, formative, summative,
goal-based, process, outcomes, etc. The type of evaluation you
undertake to improve your programs depends on what you want to
learn about the program. Don't worry about what type of evaluation
you need or are doing -- worry about what you need to know to
make the program decisions you need to make, and worry about how
you can accurately collect and understand that information.</p>


<hr align="left" />
<a name="anchor1577333"></a>

<h2>Where Program Evaluation is Helpful</h2>

<h3>Frequent Reasons:</h3>

<p>Program evaluation can:<br />
1. Understand, verify or increase the impact of products or services
on customers or clients - These &quot;outcomes&quot; evaluations
are increasingly required by nonprofit funders as verification
that the nonprofits are indeed helping their constituents. Too
often, service providers (for-profit or nonprofit) rely on their
own instincts and passions to conclude what their customers or
clients really need and whether the products or services are providing
what is needed. Over time, these organizations find themselves
in a lot of guessing about what would be a good product or service,
and trial and error about how new products or services could be
delivered.<br />
2. Improve delivery mechanisms to be more efficient and less costly
- Over time, product or service delivery ends up to be an inefficient
collection of activities that are less efficient and more costly
than need be. Evaluations can identify program strengths and weaknesses
to improve the program.<br />
3. Verify that you're doing what you think you're doing - Typically,
plans about how to deliver services, end up changing substantially
as those plans are put into place. Evaluations can verify if the
program is really running as originally planned.</p>

<h3>Other Reasons:</h3>

<p>Program evaluation can:<br />
4. Facilitate management's really thinking about what their program
is all about, including its goals, how it meets it goals and how
it will know if it has met its goals or not.<br />
5. Produce data or verify results that can be used for public
relations and promoting services in the community. <br />
6. Produce valid comparisons between programs to decide which
should be retained, e.g., in the face of pending budget cuts.<br />
7. Fully examine and describe effective programs for duplication
elsewhere.</p>


<hr align="left" />
<a name="anchor1579318"></a>

<h2>Basic Ingredients: Organization and Program(s)</h2>

<h3>You Need An Organization:</h3>

<p>This may seem too obvious to discuss, but before an organization
embarks on evaluating a program, it should have well established
means to conduct itself as an organization, e.g., (in the case
of a nonprofit) the board should be in good working order, the
organization should be staffed and organized to conduct activities
to work toward the mission of the organization, and there should
be no current crisis that is clearly more important to address
than evaluating programs.</p>

<h3>You Need Program(s):</h3>

<p>To effectively conduct program evaluation, you should first
have programs. That is, you need a strong impression of what your
customers or clients actually need. (You may have used a needs
assessment to determine these needs -- itself a form of evaluation,
but usually the first step in a good marketing plan). Next, you
need some effective methods to meet each of those goals. These
methods are usually in the form of programs.</p>

<p>It often helps to think of your programs in terms of inputs,
process, outputs and outcomes. Inputs are the various resources
needed to run the program, e.g., money, facilities, customers,
clients, program staff, etc. The process is how the program is
carried out, e.g., customers are served, clients are counseled,
children are cared for, art is created, association members are
supported, etc. The outputs are the units of service, e.g., number
of customers serviced, number of clients counseled, children cared
for, artistic pieces produced, or members in the association.
Outcomes are the impacts on the customers or on clients receiving
services, e.g., increased mental health, safe and secure development,
richer artistic appreciation and perspectives in life, increased
effectiveness among members, etc.</p>

<br />
<hr align="left" />
<a name="anchor1578833"></a>

<h2>Planning Your Program Evaluation</h2>

<h3>Depends on What Information You Need to Make Your Decisions
and On Your Resources.</h3>

<p>Often, management wants to know everything about their products,
services or programs. However, limited resources usually force
managers to prioritize what they need to know to make current
decisions.</p>

<p>Your program evaluation plans depend on what information you
need to collect in order to make major decisions. Usually, management
is faced with having to make major decisions due to decreased
funding, ongoing complaints, unmet needs among customers and clients,
the need to polish service delivery, etc. For example, do you
want to know more about what is actually going on in your programs,
whether your programs are meeting their goals, the impact of your
programs on customers, etc? You may want other information or
a combination of these. Ultimately, it's up to you.</p>

<p>But the more focused you are about what you want to examine
by the evaluation, the more efficient you can be in your evaluation,
the shorter the time it will take you and ultimately the less
it will cost you (whether in your own time, the time of your employees
and/or the time of a consultant).</p>

<p>There are trade offs, too, in the breadth and depth of information
you get. The more breadth you want, usually the less depth you
get (unless you have a great deal of resources to carry out the
evaluation). On the other hand, if you want to examine a certain
aspect of a program in great detail, you will likely not get as
much information about other aspects of the program.</p>

<p>For those starting out in program evaluation or who have very
limited resources, they can use various methods to get a good
mix of breadth and depth of information. They can both understand
more about certain areas of their programs and not go bankrupt
doing so.</p>

<p><a name="anchor184773"></a></p>

<h3>Key Considerations:</h3>

<p>Consider the following key questions when designing a program
evaluation.<br />
1. For what purposes is the evaluation being done, i.e., what
do you want to be able to decide as a result of the evaluation?<br />
2. Who are the audiences for the information from the evaluation,
e.g., customers, bankers, funders, board, management, staff, customers,
clients, etc.<br />
3. What kinds of information are needed to make the decision you
need to make and/or enlighten your intended audiences, e.g., information
to really understand the process of the product or program (its
inputs, activities and outputs), the customers or clients who
experience the product or program, strengths and weaknesses of
the product or program, benefits to customers or clients (outcomes),
how the product or program failed and why, etc.<br />
4. From what sources should the information be collected, e.g.,
employees, customers, clients, groups of customers or clients
and employees together, program documentation, etc.<br />
5. How can that information be collected in a reasonable fashion,
e.g., questionnaires, interviews, examining documentation, observing
customers or employees, conducting focus groups among customers
or employees, etc.<br />
6. When is the information needed (so, by when must it be collected)?<br />
7. What resources are available to collect the information?</p>

<br />
<hr align="left" />

<p align="center">
<script type="text/javascript"><!--
google_ad_client = "pub-9488528525256599";
if(document.body.clientWidth && document.body.clientWidth > 1040)
{
    google_ad_slot = "2264490737";
    google_ad_width = 728;
    google_ad_height = 90;
}
else
{
    google_ad_slot = "1134376367";
    google_ad_width = 468;
    google_ad_height = 60;
}
//--></script><script type="text/javascript"
src="http://pagead2.googlesyndication.com/pagead/show_ads.js"></script></p>

<p><a name="anchor1581634"></a></p>

<h2>Some Major Types of Program Evaluation</h2>

<p>When designing your evaluation approach, it may be helpful
to review the following three types of evaluations, which are
rather common in organizations. Note that you should not design
your evaluation approach simply by choosing which of the following
three types you will use -- you should design your evaluation
approach by carefully addressing the above <a href="#anchor184773"
class="textlink">key considerations</a>.</p>

<h3>Goals-Based Evaluation</h3>

<p>Often programs are established to meet one or more specific
goals. These goals are often described in the original program
plans.</p>

<p>Goal-based evaluations are evaluating the extent to which programs
are meeting predetermined goals or objectives. Questions to ask
yourself when designing an evaluation to see if you reached your
goals, are:<br />
1. How were the program goals (and objectives, is applicable)
established? Was the process effective?<br />
2. What is the status of the program's progress toward achieving
the goals? <br />
3. Will the goals be achieved according to the timelines specified
in the program implementation or operations plan? If not, then
why?<br />
4. Do personnel have adequate resources (money, equipment, facilities,
training, etc.) to achieve the goals?<br />
5. How should priorities be changed to put more focus on achieving
the goals? (Depending on the context, this question might be viewed
as a program management decision, more than an evaluation question.)<br />
6. How should timelines be changed (be careful about making these
changes - know why efforts are behind schedule before timelines
are changed)?<br />
7. How should goals be changed (be careful about making these
changes - know why efforts are not achieving the goals before
changing the goals)? Should any goals be added or removed? Why?<br />
8. How should goals be established in the future?</p>

<h3>Process-Based Evaluations</h3>

<p>Process-based evaluations are geared to fully understanding
how a program works -- how does it produce that results that it
does. These evaluations are useful if programs are long-standing
and have changed over the years, employees or customers report
a large number of complaints about the program, there appear to
be large inefficiencies in delivering program services and they
are also useful for accurately portraying to outside parties how
a program truly operates (e.g., for replication elsewhere). <br />
<br />
There are numerous questions that might be addressed in a process
evaluation. These questions can be selected by carefully considering
what is important to know about the program. Examples of questions
to ask yourself when designing an evaluation to understand and/or
closely examine the processes in your programs, are:<br />
1. On what basis do employees and/or the customers decide that
products or services are needed?<br />
2. What is required of employees in order to deliver the product
or services?<br />
3. How are employees trained about how to deliver the product
or services?<br />
4. How do customers or clients come into the program?<br />
5. What is required of customers or client?<br />
6. How do employees select which products or services will be
provided to the customer or client?<br />
7. What is the general process that customers or clients go through
with the product or program?<br />
8. What do customers or clients consider to be strengths of the
program? <br />
9. What do staff consider to be strengths of the product or program?<br />
10. What typical complaints are heard from employees and/or customers?<br />
11. What do employees and/or customers recommend to improve the
product or program?<br />
12. On what basis do employees and/or the customer decide that
the product or services are no longer needed?</p>

<h3>Outcomes-Based Evaluation</h3>

<p>Program evaluation with an outcomes focus is increasingly important
for nonprofits and asked for by funders.An outcomes-based evaluation
facilitates your asking if your organization is really doing the
right program activities to bring about the outcomes you believe
(or better yet, you've verified) to be needed by your clients
(rather than just engaging in busy activities which seem reasonable
to do at the time). Outcomes are benefits to clients from participation
in the program. Outcomes are usually in terms of enhanced learning
(knowledge, perceptions/attitudes or skills) or conditions, e.g.,
increased literacy, self-reliance, etc. Outcomes are often confused
with program outputs or units of services, e.g., the number of
clients who went through a program.</p>

<p>The <a href="http://www.unitedway.org/outcomes/" target="_blank"
class="textlink">United Way of America</a> (http://www.unitedway.org/outcomes/)
provides an excellent overview of outcomes-based evaluation, including
introduction to outcomes measurement, a program outcome model,
why to measure outcomes, use of program outcome findings by agencies,
eight steps to success for measuring outcomes, examples of outcomes
and outcome indicators for various programs and the resources
needed for measuring outcomes. The following information is a
top-level summary of information from this site.</p>

<p>To accomplish an outcomes-based evaluation, you should first
pilot, or test, this evaluation approach on one or two programs
at most (before doing all programs).</p>

<p>The general steps to accomplish an outcomes-based evaluation
include to:<br />
1. Identify the major outcomes that you want to examine or verify
for the program under evaluation. You might reflect on your mission
(the overall purpose of your organization) and ask yourself what
impacts you will have on your clients as you work towards your
mission. For example, if your overall mission is to provide shelter
and resources to abused women, then ask yourself what benefits
this will have on those women if you effectively provide them
shelter and other services or resources. As a last resort, you
might ask yourself, &quot;What major activities are we doing now?&quot;
and then for each activity, ask &quot;Why are we doing that?&quot;
The answer to this &quot;Why?&quot; question is usually an outcome.
This &quot;last resort&quot; approach, though, may just end up
justifying ineffective activities you are doing now, rather than
examining what you should be doing in the first place.<br />
2. Choose the outcomes that you want to examine, prioritize the
outcomes and, if your time and resources are limited, pick the
top two to four most important outcomes to examine for now.<br />
3. For each outcome, specify what observable measures, or indicators,
will suggest that you're achieving that key outcome with your
clients. This is often the most important and enlightening step
in outcomes-based evaluation. However, it is often the most challenging
and even confusing step, too, because you're suddenly going from
a rather intangible concept, e.g., increased self-reliance, to
specific activities, e.g., supporting clients to get themselves
to and from work, staying off drugs and alcohol, etc. It helps
to have a &quot;devil's advocate&quot; during this phase of identifying
indicators, i.e., someone who can question why you can assume
that an outcome was reached because certain associated indicators
were present.<br />
4. Specify a &quot;target&quot; goal of clients, i.e., what number
or percent of clients you commit to achieving specific outcomes
with, e.g., &quot;increased self-reliance (an outcome) for 70%
of adult, African American women living in the inner city of Minneapolis
as evidenced by the following measures (indicators) ...&quot;<br />
5. Identify what information is needed to show these indicators,
e.g., you'll need to know how many clients in the target group
went through the program, how many of them reliably undertook
their own transportation to work and stayed off drugs, etc. If
your program is new, you may need to evaluate the process in the
program to verify that the program is indeed carried out according
to your original plans. (Michael Patton, prominent researcher,
writer and consultant in evaluation, suggests that the most important
type of evaluation to carry out may be this implementation evaluation
to verify that your program ended up to be implemented as you
originally planned.)<br />
6. Decide how can that information be efficiently and realistically
gathered (see <a href="#anchor1665834" class="textlink">Selecting
Which Methods to Use</a> below). Consider program documentation,
observation of program personnel and clients in the program, questionnaires
and interviews about clients perceived benefits from the program,
case studies of program failures and successes, etc. You may not
need all of the above. (see Overview of <a href="#anchor1585345"
class="textlink">Methods to Collect Information</a> below).<br />
7. Analyze and report the findings (see <a href="#anchor1316141"
class="textlink">Analyzing and Interpreting Information</a> below).</p>

<br />
<hr align="left" />
<a name="anchor1585345"></a>

<h2>Overview of Methods to Collect Information</h2>

<p>The following table provides an overview of the major methods
used for collecting data during evaluations.<br />
<table width="100%" height="631" border="1" cellspacing="2" cellpadding="0">
  <tr>
    <td width="16%" height="24">
    <p align="center"><b>Method</b></td>
    <td width="27%">
    <p align="center"><b>Overall Purpose</b></td>
    <td width="29%">
    <p align="center"><b>Advantages</b></td>
    <td width="28%">
    <p align="center"><b>Challenges</b></td>
  </tr>
  <tr>
    <td height="116">
    questionnaires, surveys, <br />
    checklists</td> 
    <td>
    when need to quickly and/or easily get lots of information from
    people in a non threatening way</td> 
    <td>
    -can complete anonymously<br />
    -inexpensive to administer<br />
    -easy to compare and analyze<br />
    -administer to many people<br />
    -can get lots of data<br />
    -many sample questionnaires already exist</td> 
    <td>
    -might not get careful feedback<br />
    -wording can bias client's responses<br />
    -are impersonal<br />
    -in surveys, may need sampling expert<br />
    - doesn't get full story</td> 
  </tr>
  <tr>
    <td height="108">
    interviews</td> 
    <td>
    when want to fully understand someone's impressions or experiences,
    or learn more about their answers to questionnaires</td> 
    <td>
    -get full range and depth of information<br />
    -develops relationship with client<br />
    -can be flexible with client</td> 
    <td>
    -can take much time<br />
    -can be hard to analyze and compare<br />
    -can be costly<br />
    -interviewer can bias client's responses</td> 
  </tr>
  <tr>
    <td height="116">
    documentation review</td> 
    <td>
    when want impression of how program operates without interrupting
    the program; is from review of applications, finances, memos,
    minutes, etc.</td> 
    <td>
    -get comprehensive and historical information<br />
    -doesn't interrupt program or client's routine in program<br />
    -information already exists<br />
    -few biases about information</td> 
    <td>
    -often takes much time<br />
    -info may be incomplete<br />
    -need to be quite clear about what looking for<br />
    -not flexible means to get data; data restricted to what already
    exists</td> 
  </tr>
  <tr>
    <td height="24">
    observation</td> 
    <td>
    to gather accurate information about how a program actually operates,
    particularly about processes</td> 
    <td>
    -view operations of a program as they are actually occurring<br />
    -can adapt to events as they occur</td> 
    <td>
    -can be difficult to interpret seen behaviors<br />
    -can be complex to categorize observations<br />
    -can influence behaviors of program participants<br />
    -can be expensive</td> 
  </tr>
  <tr>
    <td height="124">
    focus groups</td> 
    <td>
    explore a topic in depth through group discussion, e.g., about
    reactions to an experience or suggestion, understanding common
    complaints, etc.; useful in evaluation and marketing</td> 
    <td>
    -quickly and reliably get common impressions <br />
    -can be efficient way to get much range and depth of information
    in short time<br />
    - can convey key information about programs</td> 
    <td>
    -can be hard to analyze responses<br />
    -need good facilitator for safety and closure<br />
    -difficult to schedule 6-8 people together</td> 
  </tr>
  <tr>
    <td height="100">
    case studies</td> 
    <td>
    to fully understand or depict client's experiences in a program,
    and conduct comprehensive examination through cross comparison
    of cases</td> 
    <td>
    -fully depicts client's experience in program input, process
    and results<br />
    -powerful means to portray program to outsiders</td> 
    <td>
    -usually quite time consuming to collect, organize and describe
<br />
    -represents depth of information, rather than breadth</td> 
  </tr>
</table></p>

<h4>Also see:</h4>

<p>
<a href="http://managementhelp.org/businessresearch/appreciative-inquiry.htm">Appreciative
Inquiry</a><br />
<a href="http://managementhelp.org/businessresearch/surveys.htm">Survey
Design</a><br />
</p>

<p><a name="anchor958083"></a></p>

<h3>Ethics: Informed Consent from Program Participants</h3>

<p>Note that if you plan to include in your evaluation, the focus
and reporting on personal information about customers or clients
participating in the evaluation, then you should first gain their
consent to do so. They should understand what you're doing with
them in the evaluation and how any information associated with
them will be reported. You should clearly convey terms of confidentiality
regarding access to evaluation results. They should have the right
to participate or not. Have participants review and sign an informed
consent form. See the <a href="http://managementhelp.org/businessresearch/consent-form.htm"
class="textlink">sample informed-consent form</a>.</p>

<h3>How to Apply Certain Methods</h3>

<p>
<a href="http://managementhelp.org/businessresearch/questionaires.htm">Purposes
and Formats of Questions</a>
<a href="http://managementhelp.org/businessresearch/questionaires.htm">Developing
Questionnaires</a><br />
<a href="http://managementhelp.org/businessresearch/interviews.htm">Conducting
Interviews</a><br />
<a href="http://managementhelp.org/businessresearch/focus-groups.htm">Conducting
Focus Groups</a><br />
<a href="http://managementhelp.org/businessresearch/case-studies.htm">Developing
Case Studies</a></p>


<hr align="left" />
<a name="anchor1665834"></a>

<h2>Selecting Which Methods to Use</h2>

<h3>Overall Goal in Selecting Methods:</h3>

<p>The overall goal in selecting evaluation method(s) is to get
the most useful information to key decision makers in the most
cost-effective and realistic fashion. Consider the following questions:<br />
1. What information is needed to make current decisions about
a product or program?<br />
2. Of this information, how much can be collected and analyzed
in a low-cost and practical manner, e.g., using questionnaires,
surveys and checklists?<br />
3. How accurate will the information be (reference the above table
for disadvantages of methods)?<br />
4. Will the methods get all of the needed information?<br />
5. What additional methods should and could be used if additional
information is needed?<br />
6. Will the information appear as credible to decision makers,
e.g., to funders or top management?<br />
7. Will the nature of the audience conform to the methods, e.g.,
will they fill out questionnaires carefully, engage in interviews
or focus groups, let you examine their documentations, etc.?<br />
8. Who can administer the methods now or is training required?<br />
9. How can the information be analyzed?</p>

<p>Note that, ideally, the evaluator uses a combination of methods,
for example, a questionnaire to quickly collect a great deal of
information from a lot of people, and then interviews to get more
in-depth information from certain respondents to the questionnaires.
Perhaps case studies could then be used for more in-depth analysis
of unique and notable cases, e.g., those who benefited or not
from the program, those who quit the program, etc.</p>

<h3>Four Levels of Evaluation:</h3>

<p>There are four levels of evaluation information that can be
gathered from clients, including getting their:<br />
1. reactions and feelings (feelings are often poor indicators
that your service made lasting impact)<br />
2. learning (enhanced attitudes, perceptions or knowledge)<br />
3. changes in skills (applied the learning to enhance behaviors)<br />
4. effectiveness (improved performance because of enhanced behaviors)</p>

<p>Usually, the farther your evaluation information gets down
the list, the more useful is your evaluation. Unfortunately, it
is quite difficult to reliably get information about effectiveness.
Still, information about learning and skills is quite useful.</p>


<hr align="left" />
<a name="anchor1316141"></a>

<h2>Analyzing and Interpreting Information</h2>

<p>Analyzing quantitative and qualitative data is often the topic
of advanced research and evaluation methods. There are certain
basics which can help to make sense of reams of data.</p>

<p><b>Always start with your evaluation goals:</b><br />
When analyzing data (whether from questionnaires, interviews,
focus groups, or whatever), always start from review of your evaluation
goals, i.e., the reason you undertook the evaluation in the first
place. This will help you organize your data and focus your analysis.
For example, if you wanted to improve your program by identifying
its strengths and weaknesses, you can organize data into program
strengths, weaknesses and suggestions to improve the program.
If you wanted to fully understand how your program works, you
could organize data in the chronological order in which clients
go through your program. If you are conducting an outcomes-based
evaluation, you can categorize data according to the indicators
for each outcome.</p>

<p><b>Basic analysis of &quot;quantitative&quot; information</b>
(for information other than commentary, e.g., ratings, rankings,
yes's, no's, etc.):<br />
1. Make copies of your data and store the master copy away. Use
the copy for making edits, cutting and pasting, etc.<br />
2. Tabulate the information, i.e., add up the number of ratings,
rankings, yes's, no's for each question. <br />
3. For ratings and rankings, consider computing a mean, or average,
for each question. For example, &quot;For question #1, the average
ranking was 2.4&quot;. This is more meaningful than indicating,
e.g., how many respondents ranked 1, 2, or 3. <br />
4. Consider conveying the range of answers, e.g., 20 people ranked
&quot;1&quot;, 30 ranked &quot;2&quot;, and 20 people ranked &quot;3&quot;.</p>

<p><b>Basic analysis of &quot;qualitative&quot; information</b>
(respondents' verbal answers in interviews, focus groups, or written
commentary on questionnaires):<br />
1. Read through all the data.<br />
2. Organize comments into similar categories, e.g., concerns,
suggestions, strengths, weaknesses, similar experiences, program
inputs, recommendations, outputs, outcome indicators, etc.<br />
3. Label the categories or themes, e.g., concerns, suggestions,
etc.<br />
4. Attempt to identify patterns, or associations and causal relationships
in the themes, e.g., all people who attended programs in the evening
had similar concerns, most people came from the same geographic
area, most people were in the same salary range, what processes
or events respondents experience during the program, etc.<br />
4. Keep all commentary for several years after completion in case
needed for future reference.</p>

<h3>Interpreting Information:</h3>

<p>1. Attempt to put the information in perspective, e.g., compare
results to what you expected, promised results; management or
program staff; any common standards for your services; original
program goals (especially if you're conducting a program evaluation);
indications of accomplishing outcomes (especially if you're conducting
an outcomes evaluation); description of the program's experiences,
strengths, weaknesses, etc. (especially if you're conducting a
process evaluation).<br />
2. Consider recommendations to help program staff improve the
program, conclusions about program operations or meeting goals,
etc.<br />
3. Record conclusions and recommendations in a report document,
and associate interpretations to justify your conclusions or recommendations.</p>


<hr align="left" />
<a name="anchor4293321196"></a>

<h2>Reporting Evaluation Results</h2>

<p>1.The level and scope of content depends on to whom the report
is intended, e.g., to bankers, funders, employees, customers,
clients, the public, etc.<br />
2. Be sure employees have a chance to carefully review and discuss
the report. Translate recommendations to action plans, including
who is going to do what about the program and by when.<br />
3. Bankers or funders will likely require a report that includes
an executive summary (this is a summary of conclusions and recommendations,
not a listing of what sections of information are in the report
-- that's a table of contents); description of the organization
and the program under evaluation; explanation of the evaluation
goals, methods, and analysis procedures; listing of conclusions
and recommendations; and any relevant attachments, e.g., inclusion
of evaluation questionnaires, interview guides, etc. The banker
or funder may want the report to be delivered as a presentation,
accompanied by an overview of the report. Or, the banker or funder
may want to review the report alone.<br />
4. Be sure to record the evaluation plans and activities in an
evaluation plan which can be referenced when a similar program
evaluation is needed in the future.</p>

<p><a name="anchor9055"></a></p>

<h3>Contents of an Evaluation Report -- Example</h3>

<p>An example of evaluation report contents is included later
on below in this document. Click <a href="#anchor1586742" class="textlink">Contents
of an Evaluation Plan</a> but, don't forget to look at the next
section &quot;Who Should Carry Out the Evaluation&quot;.</p>


<hr align="left" />
<a name="anchor4294569952"></a>

<h2>Who Should Carry Out the Evaluation?</h2>

<p>Ideally, management decides what the evaluation goals should
be. Then an evaluation expert helps the organization to determine
what the evaluation methods should be, and how the resulting data
will be analyzed and reported back to the organization. Most organizations
do not have the resources to carry out the ideal evaluation.</p>

<p>Still, they can do the 20% of effort needed to generate 80%
of what they need to know to make a decision about a program.
If they can afford any outside help at all, it should be for identifying
the appropriate evaluation methods and how the data can be collected.
The organization might find a less expensive resource to apply
the methods, e.g., conduct interviews, send out and analyze results
of questionnaires, etc.</p>

<p>If no outside help can be obtained, the organization can still
learn a great deal by applying the methods and analyzing results
themselves. However, there is a strong chance that data about
the strengths and weaknesses of a program will not be interpreted
fairly if the data are analyzed by the people responsible for
ensuring the program is a good one. Program managers will be &quot;policing&quot;
themselves. This caution is not to fault program managers, but
to recognize the strong biases inherent in trying to objectively
look at and publicly (at least within the organization) report
about their programs. Therefore, if at all possible, have someone
other than the program managers look at and determine evaluation
results.</p>


<hr align="left" />
<a name="anchor1586742"></a>

<h2>Contents of an Evaluation Plan</h2>

<p>Develop an evaluation plan to ensure your program evaluations
are carried out efficiently in the future. Note that bankers or
funders may want or benefit from a copy of this plan.</p>

<p>Ensure your evaluation plan is documented so you can regularly
and efficiently carry out your evaluation activities. Record enough
information in the plan so that someone outside of the organization
can understand what you're evaluating and how. Consider the following
format for your report:<br />
1. Title Page (name of the organization that is being, or has
a product/service/program that is being, evaluated; date)<br />
2. Table of Contents<br />
3. Executive Summary (one-page, concise overview of findings and
recommendations)<br />
4. Purpose of the Report (what type of evaluation(s) was conducted,
what decisions are being aided by the findings of the evaluation,
who is making the decision, etc.)<br />
5. Background About Organization and Product/Service/Program that
is being evaluated<br />
a) Organization Description/History<br />
b) Product/Service/Program Description (that is being evaluated)<br />
i) Problem Statement (in the case of nonprofits, description of
the community need that is being met by the product/service/program)<br />
ii) Overall Goal(s) of Product/Service/Program <br />
iii) Outcomes (or client/customer impacts) and Performance Measures
(that can be measured as indicators toward the outcomes)<br />
iv) Activities/Technologies of the Product/Service/Program (general
description of how the product/service/program is developed and
delivered)<br />
v) Staffing (description of the number of personnel and roles
in the organization that are relevant to developing and delivering
the product/service/program)<br />
6) Overall Evaluation Goals (eg, what questions are being answered
by the evaluation)<br />
7) Methodology <br />
a) Types of data/information that were collected<br />
b) How data/information were collected (what instruments were
used, etc.)<br />
c) How data/information were analyzed<br />
d) Limitations of the evaluation (eg, cautions about findings/conclusions
and how to use the findings/conclusions, etc.)<br />
8) Interpretations and Conclusions (from analysis of the data/information)<br />
9) Recommendations (regarding the decisions that must be made
about the product/service/program)<br />
Appendices: content of the appendices depends on the goals of
the evaluation report, eg.:<br />
a) Instruments used to collect data/information<br />
b) Data, eg, in tabular format, etc.<br />
c) Testimonials, comments made by users of the product/service/program<br />
d) Case studies of users of the product/service/program<br />
e) Any related literature</p>


<hr align="left" />
<a name="anchor1587540"></a>

<h2>Pitfalls to Avoid</h2>

<p>1. Don't balk at evaluation because it seems far too &quot;scientific.&quot;
It's not. Usually the first 20% of effort will generate the first
80% of the plan, and this is far better than nothing.<br />
2. There is no &quot;perfect&quot; evaluation design. Don't worry
about the plan being perfect. It's far more important to do something,
than to wait until every last detail has been tested.<br />
3. Work hard to include some interviews in your evaluation methods.
Questionnaires don't capture &quot;the story,&quot; and the story
is usually the most powerful depiction of the benefits of your
services.<br />
4. Don't interview just the successes. You'll learn a great deal
about the program by understanding its failures, dropouts, etc.<br />
5. Don't throw away evaluation results once a report has been
generated. Results don't take up much room, and they can provide
precious information later when trying to understand changes in
the program.</p>

<br />
<hr align="left" />

<p><a name="anchor54478"></a></p>

<h2>Online Guides</h2>

<p>
<a href="http://www.tgci.com/magazine/03fall/guide1.asp" target="_blank">Basic
Guide to Program Evaluation</a><br />
<a href="http://www.acf.hhs.gov/programs/opre/other_resrch/pm_guide_eval/index.html"
target="_blank">Program Manager's Guide to Evaluation</a><br />
<a href="http://ask.hrsa.gov/detail_materials.cfm?ProdID=1566"
target="_blank">Analytical Methods in Maternal and Child Health</a><br />
<a href="http://managementhelp.org/evaluation/program-evaluation-guide.htm">Basic
Guide to Program Evaluation</a><br />
<a href="http://www.liveunited.org/outcomes/" target="_blank">Outcome
Measurement Resource Network (lots of useful links from the United
Way)</a><br />
<a href="http://outreach.missouri.edu/staff/programdev/plm/" 
target="_blank">What is a Program Logic Model? (logic model captures
inputs, activities, outputs, outcomes)</a><br />
<a href="http://www.nationalserviceresources.org/files/legacy/filemanager/download/ProgramMgmt/Outcome_Measurement_Showing_Results_Nonprofit_Sector.pdf"
target="_blank">Outcome Measurement: Showing Results (wonderful
overview of outcomes, myths, etc.)</a><br />
<a href="http://bcn.boulder.co.us/aerie/evaluation/evaluate.htm"
target="_blank">Economic Outcomes Evaluation Methodology (a little
advanced?)</a><br />
<a href="http://www.wkkf.org/knowledge-center/resources/2010/W-K-Kellogg-Foundation-Evaluation-Handbook.aspx" target="_blank">W.K. Kellogg Foundation Evaluation Handbook</a></p>

<a name="anchor3873957"></a>
<h2>Outcomes-Evaluation</h2>

<p>
<a href="http://managementhelp.org/evaluation/outcomes-evaluation-guide.htm"
target="_blank">Basic Guide to Outcomes-Based Evaluation for Nonprofit
Organizations with Very Limited Resources</a><br />
<a href="http://www.liveunited.org/outcomes/" target="_blank">Outcome 
Measurement Resource Network (lots of useful links from the United Way)</a><br />
<a href="http://outreach.missouri.edu/staff/programdev/plm/" 
target="_blank">What is a Program Logic Model? (logic model captures
inputs, activities, outputs, outcomes)</a><br />
<a href="http://nationalserviceresources.org/files/legacy/filemanager/download/ProgramMgmt/Outcome_Measurement_Showing_Results_Nonprofit_Sector.pdf"
target="_blank">Outcome Measurement: Showing Results (wonderful
overview of outcomes, myths, etc.)</a><br />
<a href="http://bcn.boulder.co.us/aerie/evaluation/evaluate.htm"
target="_blank">Economic Outcomes Evaluation Methodology (a little
advanced?)</a><br />
<a href="http://aea365.org/blog/" target="_blank">Paul Duignan
on DoView and Visually Representing Outcomes</a><br />
<a href="http://www.uwex.edu/ces/pdande/evaluation/evallogicmodelexamples.html"
target="_blank">Evaluation Logic Model Examples</a><br />
<a href="http://portal.whatworks.org/home.aspx" target="_blank">Impact Measurement Framework</a><br />
<a href="http://portal.whatworks.org/programs.aspx" target="_blank">Outcomes Framework Browser</a></p>

<p><a name="anchor227968"></a></p>

<h2>General Resources</h2>

<p><i>(Thanks to Gene Shackman for suggesting many of the following
resources.)<br /></i>
<a href="http://www.eval.org/" target="_blank">American Evaluation
Association</a><br />
<a href="http://www.ojp.usdoj.gov/BJA/evaluation/" target="_blank">Bureau
of Justice Assistance Evaluation Website</a><br />
<a href="http://www.mhsip.org/library.html" target="_blank">Online
library of resources for evaluation of mental health programs</a><br />
<a href="http://gsociology.icaap.org/methods/" target="_blank">Free
Resources for Program Evaluation and Social Research Methods</a><br />
<a href="http://www.evaluation.lars-balzer.name/links/" target="_blank">Evaluation
portal and links collection</a>
<a href="http://managementhelp.org/evaluation/index.htm#anchor227968">Guides
for many types of evaluation</a><br />
<a href="http://aea365.org/blog/" target="_blank">Susan Kistler
on Online Journals for Evaluators</a><br />
<a href="http://www.acf.hhs.gov/programs/opre/other_resrch/pm_guide_eval/index.html"
target="_blank">Program Manager's Guide to Evaluation</a><br />
<a href="http://ask.hrsa.gov/detail_materials.cfm?ProdID=1566"
target="_blank">Analytical Methods in Maternal and Child Health</a><br />
<a href="http://gsociology.icaap.org/methods/BasicguidesHandouts.html"
target="_blank">What is program evaluation: A set of beginners
guides</a><br />
<a href="http://www.jrsa.org/pubs/juv-justice/evaluator.pdf" 
target="_blank">Hiring and Working With an Evaluator</a><br />
<a href="http://blogs.hbr.org/pallotta/2010/11/our-ineffectiveness-at-measuri.html"
target="_blank">Our Ineffectiveness at Measuring Effectiveness</a><br />
<a href="http://www.urban.org/center/cnp/projects/outcomeindicators.cfm"
target="_blank">Outcome Indicators Project</a><br />
<a href="http://aea365.org/blog/" target="_blank">Sudharshan Seshadri
on Resources for Program Evaluation</a><br />
<a href="http://facilitationprocess.com/the-critical-need-for-program-accountabiltiy-evalaution"
target="_blank">The Critical Need for Program Accountability &amp;
Evaluation</a><br />
<a href="http://managementhelp.org/blogs/nonprofit-capacity-building/2011/11/29/how-to-address-fears-about-program-evaluation/" target="_blank">How to Address Fears about Program Evaluation</a><br />
<a href="http://managementhelp.org/blogs/nonprofit-capacity-building/2011/11/21/how-to-maximize-funding-by-tapping-into-hidden-potential-program-evaluation/" target="_blank">How to Maximize Funding by Tapping into Hidden Potential: Program Evaluation</a><br />
<a href="http://www.communitysolutions.ca/pdf/planning-evaluation-diagram.pdf" target="_blank">How Traditional Planning
and Evaluation Interact</a><br />
<a href="http://managementhelp.org/blogs/nonprofit-capacity-building/2012/01/08/four-differences-between-research-and-program-evaluation/" target="_blank">Four Differences between Research and Program Evaluation</a><br />
<a href="http://managementhelp.org/blogs/nonprofit-capacity-building/2012/01/23/which-is-more-important—the-means-or-the-ends/">Which Is More Important—the Means or the Ends? Process, Impact and Outcome Evaluations</a><br />
<a href="http://managementhelp.org/blogs/nonprofit-capacity-building/2012/02/06/a-guide-to-navigating-the-evaluation-maze-“a-framework-for-evaluation”-from-the-centers-for-disease-control-and-prevention-cdc-part-1/" target="_blank">A Guide to Navigating the Evaluation Maze: “A Framework for Evaluation” from the CDC, Part 1</a><br />
<a href="http://managementhelp.org/blogs/nonprofit-capacity-building/2012/03/08/a-guide-to-navigating-the-evaluation-maze-“a-framework-for-evaluation”-from-the-centers-for-disease-control-and-prevention-cdc-part-2/" target="_blank">A Guide to Navigating the Evaluation Maze: “A Framework for Evaluation” from the CDC, Part 2</a><br />
<a href="http://managementhelp.org/blogs/nonprofit-capacity-building/2012/04/10/tips-on-conducting-interviews-for-program-evaluation-part-1/" target="_blank">Tips on How to Conduct Interviews for Program Evaluation (part 1)</a><br />
<a href="http://managementhelp.org/blogs/nonprofit-capacity-building/2012/05/08/tips-on-how-to-conduct-interviews-for-program-evaluation-part-2/" target="_blank">Tips on How to Conduct Interviews for Program Evaluation (Part 2)</a><br />
<a href="http://www.gatesfoundation.org/learning/documents/wwl-report-measuring-estimating-social-value-creation.pdf" target="_blank">Measuring and/or Estimating Social Value Creation: Insights Into Eight Integrated Cost Approaches</a><br />
<a href="http://managementhelp.org/blogs/nonprofit-capacity-building/2012/05/29/how-to-evaluate-on-a-budget-diy-or-outsource/" target="_blank">How to Evaluate on a Budget: DIY or Outsource?</a></p>


<hr align="left" /><p align="center">
<a href="http://managementhelp.org/email/form-to-add-content.htm">
<img src="http://managementhelp.org/images/insertlink-btn.gif" alt="Submit a link" border="0" height="25" width="125" /></a>
</p><hr align="left" />

<a name="anchor1029384756"></a>

<h2>For the Category of Evaluations (Many Kinds):</h2>

<p>To round out your knowledge of this Library topic, you may
want to review some related topics, available from the link below.
Each of the related topics includes free, online resources.<br />
<br />
Also, scan the Recommended Books listed below. They have been
selected for their relevance and highly practical nature.</p>

<h2><a href="http://managementhelp.org/relatedtopics/eval-rlt.htm">Related
Library Topics</a></h2>


<!--#include virtual="/books/book-include/eval-bks-incl.htm"--><br />

<!--#include virtual="/template/bottom-right.html"-->
<!--         virtual "/bizo-tags/bizo-default-tag.txt" -->

</body>
</html>
